{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uditagupta93/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['delete', 'text']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we use re package to filter out unnecessary characters, tokenize the words and convert it to lower case\n",
    "def tokens(text):\n",
    "    return re.findall('[a-z]+', text.lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we use re package to filter out unnecessary characters, tokenize the words and convert it to lower case\n",
    "def tokens_words(text):\n",
    "    x = re.findall('[a-z]+', text.lower()) \n",
    "    #print(x)\n",
    "    return str(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6488665"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = open('big.txt').read() #read all the words from big.txt\n",
    "len(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spell_errors = open(\"spell-errors.txt\").read()\n",
    "#len(spell_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7506\n"
     ]
    }
   ],
   "source": [
    "delete = [':','?',';',',','>']\n",
    "\n",
    "def load_spell_errors(filename, sep=':'):\n",
    "    spell_errors_dict = {}\n",
    "    \"\"\"Return a Counter initialized from key-value pairs, \n",
    "    one on each line of filename.\"\"\"\n",
    "    for line in open(filename):\n",
    "        x = line.split(sep)\n",
    "        key = x[0]\n",
    "        if key not in delete:\n",
    "            key = tokens_words(key)\n",
    "            val = x[1:]\n",
    "            val = val[0].split(',')\n",
    "            #print(val)\n",
    "            for i in range(len(val)):\n",
    "\n",
    "                val[i] = val[i].strip()\n",
    "                val[i] = tokens_words(val[i])\n",
    "                #print(i)\n",
    "\n",
    "\n",
    "            spell_errors_dict[key]=val\n",
    "\n",
    "    return spell_errors_dict\n",
    "\n",
    "\n",
    "spell_errors_dict = load_spell_errors('spell-errors.txt')\n",
    "print(len(spell_errors_dict))\n",
    "#print(spell_errors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORDS = tokens(TEXT)\n",
    "counts_sum = len(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 80030), ('of', 40025), ('and', 38313), ('to', 28766), ('in', 22050), ('a', 21155), ('that', 12512), ('he', 12401), ('was', 11410), ('it', 10681)]\n"
     ]
    }
   ],
   "source": [
    "COUNTS = Counter(WORDS)\n",
    "\n",
    "print(COUNTS.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_counts(filename, sep='\\t'):\n",
    "    \"\"\"Return a Counter initialized from key-value pairs, \n",
    "    one on each line of filename.\"\"\"\n",
    "    C = Counter()\n",
    "    for line in open(filename):\n",
    "        key, count = line.split(sep)\n",
    "        C[key] = int(count)\n",
    "    return C\n",
    "\n",
    "\n",
    "COUNTS1 = load_counts('count_1w.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 23135851162), ('of', 13151942776), ('and', 12997637966), ('to', 12136980858), ('a', 9081174698), ('in', 8469404971), ('for', 5933321709), ('is', 4705743816), ('on', 3750423199), ('that', 3400031103)]\n"
     ]
    }
   ],
   "source": [
    "print(COUNTS1.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COUNTS1_dict = dict(COUNTS1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588124220187\n"
     ]
    }
   ],
   "source": [
    "# get the overall corpus count\n",
    "counts1_sum = 0\n",
    "for val in COUNTS1_dict.values():\n",
    "    counts1_sum += val\n",
    "    \n",
    "print(counts1_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Checker\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    #Generating all the words with edit distance of 0, 1 & 2\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    #print(candidates)\n",
    "    #print(COUNTS.get)\n",
    "    #print(max(candidates, key=COUNTS.get))\n",
    "    return(candidates)\n",
    "    #return max(candidates, key=COUNTS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    #Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for `edits1(word)`: the set of candidate words that are one edit away. For example, given `\"wird\"`, this would include `\"weird\"` (inserting an `e`) and `\"word\"` (replacing a `i` with a `o`), and also `\"iwrd\"` (transposing `w` and `i`; then `known` can be used to filter this out of the set of final candidates). How could we get them?  One way is to *split* the original word in all possible places, each split forming a *pair* of words, `(a, b)`, before and after the place, and at each place, either delete, transpose, replace, or insert a letter:\n",
    "\n",
    "<table>\n",
    "  <tr><td> pairs: <td><tt> Ø+wird <td><tt> w+ird <td><tt> wi+rd <td><tt>wir+d<td><tt>wird+Ø<td><i>Notes:</i><tt> (<i>a</i>, <i>b</i>)</tt> pair</i>\n",
    "  <tr><td> deletions: <td><tt>Ø+ird<td><tt> w+rd<td><tt> wi+d<td><tt> wir+Ø<td><td><i>Delete first char of b</i>\n",
    "  <tr><td> transpositions: <td><tt>Ø+iwrd<td><tt> w+rid<td><tt> wi+dr</tt><td><td><td><i>Swap first two chars of b\n",
    "  <tr><td> replacements: <td><tt>Ø+?ird<td><tt> w+?rd<td><tt> wi+?d<td><tt> wir+?</tt><td><td><i>Replace char at start of b\n",
    "  <tr><td> insertions: <td><tt>Ø+?+wird<td><tt> w+?+ird<td><tt> wi+?+rd<td><tt> wir+?+d<td><tt> wird+?+Ø</tt><td><i>Insert char between a and b\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'access', 'acres', 'across', 'actress', 'caress'}]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(correct, tokens('acress')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word = 'Wird:'\n",
    "def calculate_candidates(word):\n",
    "    word = tokens_words(word)\n",
    "    final_candidate_list =[]\n",
    "    candidates = correct(tokens_words(word))\n",
    "    #print(candidates)\n",
    "\n",
    "    for candidate in candidates:\n",
    "        if candidate in spell_errors_dict:\n",
    "            for val1 in  spell_errors_dict[candidate]:\n",
    "                if val1 == word:\n",
    "                    final_candidate_list.append(candidate)\n",
    "\n",
    "    if(len(final_candidate_list)) == 0:\n",
    "        final_candidate_list = list(candidates)\n",
    "        final_candidate_list = list(set(final_candidate_list))\n",
    "    else:\n",
    "        final_candidate_list = list(set(final_candidate_list))\n",
    "\n",
    "    #final_candidate_list = list(set(final_candidate_list))              \n",
    "    #print(final_candidate_list)\n",
    "    return final_candidate_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_probability(final_candidate_list):\n",
    "    probability_of_w = {}\n",
    "    \n",
    "    for candidate in final_candidate_list:\n",
    "        if candidate in COUNTS1_dict:\n",
    "            probability_of_w[candidate] = (COUNTS1_dict[candidate]/counts1_sum)\n",
    "        else:\n",
    "            probability_of_w[candidate] = (COUNTS[candidate]/counts_sum)\n",
    "\n",
    "    #print(probability_of_w)\n",
    "    return probability_of_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_probability_word(probability_of_w):\n",
    "    return max(probability_of_w.items(), key = operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    }
   ],
   "source": [
    "final_candidate_list = calculate_candidates('faulse')\n",
    "probability_of_w = find_probability(final_candidate_list)\n",
    "print(max_probability_word(probability_of_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make the output prettier than that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID       WRONG\n",
      "0   0  contenpted\n",
      "1   1    begining\n",
      "2   2     problam\n",
      "3   3      dirven\n",
      "4   4     exstacy\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "print(test_df.head(5))\n",
    "test_df.drop('ID', axis=1)\n",
    "test_list = test_df['WRONG'].tolist()\n",
    "#print(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contented', 'beginning', 'problem', 'driven', 'ecstasy', 'juice', 'locally', 'compare', 'pronunciation', 'transportibility', 'miniscule', 'independent', 'arranged', 'poetry', 'level', 'basically', 'triangular', 'unexpected', 'stanerdizing', 'variable', 'further', 'monitoring', 'biscuits', 'available', 'separate', 'necessary', 'definition', 'receipt', 'remind', 'initials', 'magnificent', 'aunt', 'initial', 'the', 'experiences', 'built', 'totally', 'understand', 'southern', 'definitely', 'visited', 'voluntary', 'meant', 'receive', 'sources', 'whether', 'useful', 'literature', 'valuable', 'delicate', 'clerical', 'splendid', 'between', 'completely', 'account', 'cemetery', 'special', 'latest', 'perhaps', 'member', 'chapter', 'cake', 'various', 'february', 'pretend', 'choosing', 'wrote', 'particular', 'awful', 'arrangement', 'challenges', 'laugh', 'often', 'someone', 'personnel', 'unique', 'diagrammaticaally', 'description', 'poems', 'purple', 'decide', 'articles', 'position', 'extended', 'hierachial', 'really', 'voting', 'committee', 'wanted', 'benefits', 'definitions', 'scissors', 'levels', 'parallel', 'accommodation', 'planed', 'hierarchy', 'transferred', 'mines', 'arranging', 'accusing', 'stomach', 'unfortunately', 'considerable', 'access', 'singular', 'scarcely', 'questionnaire', 'experience', 'possible', 'refreshment', 'embargo', 'visitors', 'axillary', 'decided', 'benefit', 'consider', 'fails', 'career', 'occurrence', 'certain', 'poems', 'lieu', 'establishing', 'different', 'lines', 'extremely', 'addresable', 'gallery', 'central', 'families', 'bicycle', 'choice', 'opposite', 'curtains', 'address', 'liaison', 'management', 'inconvenient', 'variant', 'supersede', 'appeal', 'employees', 'encourage', 'permanent', 'mathematically', 'data', 'permanently', 'hierachial', 'proviso', 'moving', 'allow', 'credit', 'available', 'traditionally', 'adabtable', 'latter', 'graphicaly', 'eventually', 'doubt', 'academic', 'subsequent', 'misleading', 'ordinary', 'associated', 'voluntary', 'enormously', 'table', 'useful', 'thermawere', 'present', 'between', 'appreciation', 'accept', 'suffering', 'comparison', 'majority', 'umbrella', 'discipline', 'arrangement', 'prepared', 'sufficient', 'wanted', 'orentated', 'input', 'pitting', 'profit', 'agencies', 'accumulated', 'manual', 'representative', 'employed', 'contused', 'dragged', 'unresloved', 'compared', 'now', 'announcing', 'unequaled', 'currently', 'titles', 'reaching', 'approached', 'suggestion', 'avaiblity', 'detention', 'cancellation', 'transactions', 'believe', 'operations', 'resulting', 'decides', 'unequivocal', 'minutes', 'understandable', 'dependence', 'supposedly', 'subtrcat', 'sheets', 'consist', 'imidatly', 'exponentualy', 'overall', 'indeed', 'separated', 'length', 'family', 'nessisitates', 'described', 'primarily', 'suggested', 'possibilities', 'building', 'universally', 'unessasarily', 'beginning', 'and', 'comments', 'the', 'declarations', 'supervision', 'questionnaire', 'develop', 'unavailble', 'appointments', 'variable', 'vegetative', 'equaled', 'bonus', 'demands', 'scrutinized', 'recently', 'clerk', 'adequate', 'feeling', 'familiar', 'accepted', 'accusing', 'separation', 'equities', 'receiving', 'procedure', 'conceived', 'generate', 'flexible', 'analysis', 'particularly', 'routine', 'nation', 'gaining', 'geneva', 'interesting', 'surroundings', 'opportunity', 'functional', 'together', 'approach', 'scheme', 'occurred', 'preffeson', 'recommend', 'examine', 'their', 'readjusted', 'build', 'organisation', 'university', 'utilised', 'obtaining', 'commercial', 'embelishing', 'description', 'edition', 'forbidden', 'applicable', 'committee', 'search', 'necessity', 'similar', 'luckily', 'economic', 'again', 'patterns', 'analyzing', 'variety', 'widely', 'arrangements', 'challenge', 'forth', 'accounts', 'politics', 'decisions', 'sense', 'beneficial', 'arguing', 'continually', 'aquaintances', 'extremely', 'excessively', 'composed', 'seen', 'nationally', 'source', 'months', 'progresses', 'were', 'brief', 'matrix', 'visitor', 'adequately', 'security', 'successive', 'consisting', 'pere', 'commit', 'except', 'conversely', 'decisions', 'union', 'when', 'shown', 'will', 'suited', 'necasery', 'essential', 'government', 'unessessay', 'disaggreagte', 'argument', 'chooses', 'conference', 'proceeding', 'profits', 'apparent', 'ideally', 'ability', 'responsibilities', 'are', 'many', 'surveying', 'provide', 'within', 'session', 'false', 'annual', 'summary', 'reference', 'employment', 'et', 'particular', 'surrounding', 'chose', 'journalism', 'annoying', 'expense', 'receive', 'however', 'supplementary', 'virtually', 'forecast', 'hearty', 'combine', 'requested', 'desperately', 'appreciated', 'safeguard', 'shortened', 'confirmation', 'advantageous', 'these', 'before', 'progression', 'component', 'numbers', 'adjournment', 'according', 'opinion', 'advice', 'career', 'resolved', 'accessability', 'negligible', 'students', 'favorable', 'technique', 'atmosphere', 'exactly', 'especially', 'proportions', 'geographical', 'encompasing', 'stopped', 'relevant', 'that', 'applied', 'eliminated', 'contains', 'conditining', 'sucssuful', 'quiet', 'aggravating', 'erroneous', 'weapons', 'corporate', 'inconceivable', 'organised', 'sensible', 'studying', 'examination', 'analysis', 'lonely', 'interest', 'committees', 'explaining', 'humor', 'thorns', 'timing', 'difficulty', 'careers', 'register', 'site', 'appointment', 'surveys', 'apologies', 'approximately', 'throat', 'prior', 'executed', 'anomalies', 'relatively', 'announcement', 'accompanying', 'projects', 'canalised', 'therefore', 'perametres', 'whereas', 'the', 'discretion', 'handle', 'dissapoiting', 'expansion', 'personal', 'senior', 'apologized', 'separate', 'mean', 'guideline', 'busy', 'easily', 'create', 'system', 'speaking', 'interogationg', 'drasticaly', 'difficult', 'ineffiect', 'operator', 'output', 'base', 'regained', 'controlled', 'referred', 'irrelevant', 'although', 'years', 'means', 'technique', 'neither', 'required', 'correspondence', 'interpretation', 'segment', 'throughout', 'qualities', 'financially', 'intelligence', 'receives', 'citisum', 'would', 'investigated', 'recommending', 'earliest', 'immediate']\n"
     ]
    }
   ],
   "source": [
    "correct_list =[]\n",
    "for text in test_list:\n",
    "    final_candidate_list = calculate_candidates(text)\n",
    "    probability_of_w = find_probability(final_candidate_list)\n",
    "    correct_list.append(max_probability_word(probability_of_w))\n",
    "    \n",
    "print(correct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csvfile = \"sample_test_submit1.csv\"\n",
    "\n",
    "#Assuming res is a flat list\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in correct_list:\n",
    "        #print(val)\n",
    "        writer.writerow([val])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://web.stanford.edu/class/cs276/pa/pa2.p\n",
    "\n",
    "http://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "\n",
    "http://norvig.com/ngrams/spell-errors.txt\n",
    "\n",
    "http://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb\n",
    "\n",
    "http://norvig.com/big.txt\n",
    "\n",
    "http://norvig.com/ngrams/count_1w.txt\n",
    "\n",
    "http://norvig.com/ngrams/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
